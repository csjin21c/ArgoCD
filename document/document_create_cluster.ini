# ####################################################################################################
# GigHub Actions & ArgoCD on EKS Cluster
# ####################################################################################################

# ####################################################################################################
# I. VPC Network Architecture
# ====================================================================================================
# 1. 리전 및 가용 영역 정보
# ----------------------------------------------------------------------------------------------------
# - Region: ap-northeast-1 (도쿄)
# - Availability Zones: ap-northeast-1a / ap-northeast-1c / ap-northeast-1d

# ====================================================================================================
# 2. VPC와 네트워크 기본정보
# ----------------------------------------------------------------------------------------------------
# Type            Availability Zone   Subnet ID                   CIDR 블록      비고
# ----------------------------------------------------------------------------------------------------
# vpc                                 vpc-089fe9578dcc1c00c       10.0.0.0/20   IGW
# ----------------------------------------------------------------------------------------------------
# Public Subnet   ap-northeast-1a     subnet-0b1805c1e9f180871    10.0.0.0/26   NAT,  외부 LB용
#                 ap-northeast-1c     subnet-0bb8d9b6a060a7b6b    10.0.1.0/26         외부 LB용
#                 ap-northeast-1d     subnet-0bc9e9327311ce319    10.0.2.0/26         외부 LB용
# ----------------------------------------------------------------------------------------------------
# Private Subnet  ap-northeast-1a     subnet-03d99036d72d4e535    10.0.0.64/26        내부 LB용
#                 ap-northeast-1c     subnet-0e961c601df663df9    10.0.1.64/26        내부 LB용
#                 ap-northeast-1d     subnet-0d9812b3f0d502b24    10.0.2.64/26        내부 LB용

# ====================================================================================================
# 3. VPC와 Subnet에 아래와 같이 태그를 추가합니다:
#    kubernetes.io/role/elb=1이 지정된 퍼블릭 서브넷은 외부 로드밸런서용 서브넷 대상이 되는 것으로,
# ----------------------------------------------------------------------------------------------------
# 대상 리소스          태그 키 (Key)                             태그 값 (Value)   비고
# ----------------------------------------------------------------------------------------------------
# VPC                kubernetes.io/cluster/[cluster-name]    shared          인프라 식별
# ----------------------------------------------------------------------------------------------------
# Public Subnet      kubernetes.io/cluster/[cluster-name]    shared          클러스터 소속
#                    kubernetes.io/role/elb                  1               외부 LB용
# ----------------------------------------------------------------------------------------------------
# Private Subnet     kubernetes.io/cluster/[cluster-name]    shared          클러스터 소속
#                    kubernetes.io/role/internal-elb         1               내부 LB용

# ====================================================================================================
# 4. 보안 그룹 설정
# ----------------------------------------------------------------------------------------------------
# Name              Secrets Group ID      인바운드 규칙
# ----------------------------------------------------------------------------------------------------
# csjin-default-sg  sg-025ab8cfad0cab78c  delete inbound
# csjin-ssh-sg      sg-04264d83ba971cc87  SSH(22)             User's IP
# csjin-alb-sg      sg-0dab199d320638d6a  HTTP(80)            0.0.0.0/0
#                                         HTTPS(443)          0.0.0.0/0
#                                         TCP(8000)           0.0.0.0/0
# csjin-cluster-sg  sg-0746271ec52f43ed1  TCP(30000 - 32767)  csjin-alb-sg
# csjin-mysql-sg    sg-082666b0cbf035f8e  MySQL(3306)         csjin-ssh-sg, csjin-cluster-sg

# ====================================================================================================
# 5. Application Load Balancer용 IAM Role 생성
# ----------------------------------------------------------------------------------------------------
# - 역할 이름: eksctl-csjin-cluster-Role1-1A2B2C3D4E5F
# - 정책 연결: AmazonEC2FullAccess

# ####################################################################################################
# II. EKS Cluster Creation and Management
# ====================================================================================================
# eksctl 클러스터 명령
# -------------------------------------------------------------------------------------------------------
#  단계       실행 도구                                    명령어 및 설명
#  최초 생성	 eksctl create cluster -f cluster.yaml	    YAML 설계도대로 클러스터와 노드를 처음 구축합니다.
#  설정 변경   eksctl upgrade nodegroup -f cluster.yaml   노드 그룹의 설정(버전, IAM 권한 등)을 수정하여 반영합니다.
#  개수 조정	 eksctl scale nodegroup -f cluster.yaml	    YAML에 정의된 min/max/desired 값으로 노드 대수를 맞춥니다.
#  삭제	      eksctl delete cluster -f cluster.yaml	     클러스터와 관련 리소스를 모두 삭제합니다.
# -------------------------------------------------------------------------------------------------------

# ====================================================================================================
# create_cluster.yaml 내용:
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

# 클러스터 기본 정보 설정
metadata:
  name: csjin-cluster        # 생성할 EKS 클러스터 이름 (태그와 일치해야 함)
  region: ap-northeast-1     # 클러스터가 생성될 리전 (도쿄)
  version: "1.31"            # 쿠버네티스 제어 평면 버전(1.29 ~ 1.35 지원, 1.31 권장)

# 기존 VPC 및 서브넷 활용 설정
vpc:
  id: "vpc-089fe9578dcc1c00c"  # 사용자가 UI로 생성한 기존 VPC ID
  subnets:
    # 외부 로드밸런서가 위치할 퍼블릭 서브넷 (elb 태그 필요)
    public:
      ap-northeast-1a: { id: "subnet-0b1805c1e9f180871" }
      ap-northeast-1c: { id: "subnet-0bb8d9b6a060a7b6b" }
      ap-northeast-1d: { id: "subnet-0bc9e9327311ce319" }
    # 실제 워커 노드가 위치할 프라이빗 서브넷 (internal-elb 태그 필요)
    private:
      ap-northeast-1a: { id: "subnet-03d99036d72d4e535" }
      ap-northeast-1c: { id: "subnet-0e961c601df663df9" }
      ap-northeast-1d: { id: "subnet-0d9812b3f0d502b24" }
  # VPC 서브넷 태그 자동 생성을 위한 설정 (Public 서브넷 사용)
  clusterEndpoints:
    publicAccess: true # 보안 관점에선 false 권장, 하지만 CI/CD 환경에선 true 필요(외부 접속 허용)
    privateAccess: true
  # 특정 IP만 조종실(k8s master)에 접속 가능하도록 제한, publicAccess: true 시에만 유효하나 취약점 보완할 수 있어 사용합니다.
  # publicAccessCidrs:
  #   - "210.xxx.xxx.xxx/32"
  #   - "1.2.3.4/32"

# 관리형 노드 그룹(Managed Node Group) 설정
managedNodeGroups:
  - name: csjin-cluster-ng     # 노드 그룹의 이름
    instanceType: t3.medium    # 사용할 EC2 인스턴스 타입 (사양)
    minSize: 1                 # 최소 1대 유지
    maxSize: 3                 # 최대 3대까지 확장 가능
    desiredCapacity: 2         # 초기 생성할 노드 개수
    volumeSize: 20             # 각 노드의 EBS(디스크) 크기 (GB)
    privateNetworking: true    # 노드들을 외부 인터넷에 노출되지 않는 프라이빗 서브넷에 배치
    securityGroups:
      attachIDs:
        - sg-0746271ec52f43ed1 # 첫 번째 보안 그룹: csjin-cluster-sg
        - sg-04264d83ba971cc87 # 두 번째 보안 그룹: csjin-ssh-sg, 확인 후 삭제
    iam:
      # 노드에 기본적으로 필요한 AWS 정책 권한 추가
      withAddonPolicies:
        imageBuilder: true     # ECR 이미지 접근 권한
        autoScaler: true       # Cluster Autoscaler 사용을 위한 권한
        albIngress: true       # 로드밸런서 관련 기본 권한을 노드에 자동으로 부여
        cloudWatch: true       # 로그 전송을 위한 CloudWatch 권한
        ebs: true              # EBS 볼륨 사용 권한
        appMesh: true          # 서비스 메시 사용 시 필요
        xRay: true             # 분산 트레이싱 사용 시 필요
        certManager: true      # SSL/TLS 인증서 자동 발급 권한
        externalDNS: true      # 도메인(Route53) 자동 연결 권한

# IAM OIDC 프로바이더 설정
iam:
  withOIDC: true               # 쿠버네티스 서비스 계정에 IAM 역할을 부여하기 위한 설정 (필수)
# ====================================================================================================



# ####################################################################################################
# III. 애플리케이션 배포: Nginx 웹서버 + FastAPI (로드밸런서 포함)
# ====================================================================================================
# 1. Service: 외부에서 접속 가능한 로드밸런서(대문) 정의 : web-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: web-project-svc      # 서비스 이름
spec:
  type: NodePort             # ALB는 NodePort 서비스를 바라보고 트래픽을 넘깁니다.
  selector:
    app: web-project                # 위에서 만든 nginx 라벨을 가진 파드들로 트래픽을 전달
  ports:
    - name: http               # [리스너 1] 이름 지정 (포트가 여러 개일 땐 필수)
      protocol: TCP
      port: 80                 # 로드밸런서가 외부에서 받는 포트
      targetPort: 80           # 실제 파드로 전달되는 포트
    # - name: custom-port        # [리스너 2] 추가된 리스너
    #   protocol: TCP
    #   port: 8000               # 외부에서 8080으로 접속하면
    #   targetPort: 8000           # 파드의 80번으로 전달 (포트 포워딩 역할)
---
# 2. Ingress를 통해 ALB를 생성합니다
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-project-svc         
  annotations: 
    # ALB 클래스 지정 (최신 방식은 spec.ingressClassName을 쓰지만 annotation도 작동함)
    kubernetes.io/ingress.class: alb
    
    # 외부 노출 및 타겟 설정
    alb.ingress.kubernetes.io/scheme: internet-facing   # internet-facing(외부공개)/internal(내부)
    alb.ingress.kubernetes.io/target-type: instance      # instance 또는 ip(파드 직접 지정, VPC CNI환경이 완벽해야 함)
    
    # 보안 그룹
    alb.ingress.kubernetes.io/security-groups: sg-0dab199d320638d6a
    
    # 4. 헬스체크 설정
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    
    # 기타 속성 (ALB 전용 표기법)
    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=60
spec:
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-project-svc   # 서비스 이름과 일치해야 함
                port:
                  number: 80
---
# ---------------------------------------------------------
# 2. Deployment: 애플리케이션(Nginx) 컨테이너 실행 정의
# ---------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-deployment      # 이 배포의 이름
spec:
  replicas: 2                 # [요청 개수] 처음에 띄울 파드의 개수 (요청하신 2개)
  selector:
    matchLabels:
      app: web-project              # 이 라벨이 붙은 파드들을 관리하겠다는 선언
  template:
    metadata:
      labels:
        app: web-project            # 생성될 파드에 부여할 라벨 (위의 selector와 일치해야 함)
    spec:
      containers:
      # 1번 컨테이너: Nginx (웹서버 역할)
      - name: nginx
        image: 626635419731.dkr.ecr.ap-northeast-1.amazonaws.com/csjin/nginx:latest-aws
        ports:
        - containerPort: 80    # 컨테이너 안에서 Nginx가 사용하는 포트
        resources:             # [중요] HPA가 CPU 사용량을 계산하기 위한 기준 설정
          requests:
            cpu: "100m"         # [예약] "최소한 이만큼은 보장해줘" (스케줄링 기준)
            memory: "128Mi"
          limits:
            cpu: "250m"         # [한계] "아무리 바빠도 0.5코어 이상은 쓰지마" (강제 제한)
            memory: "256Mi"
      
      # 2번 컨테이너: FastAPI (Application 역할)
      # - name: fastapi
      #   image: 626635419731.dkr.ecr.ap-northeast-1.amazonaws.com/csjin/fastapi:3.14-slim
      #   ports:
      #   - containerPort: 8000  # FastAPI가 사용하는 포트
      #   resources:
      #     requests:
      #       cpu: "100m"         # [예약] "최소한 이만큼은 보장해줘" (스케줄링 기준)
      #       memory: "128Mi"
      #     limits:
      #       cpu: "500m"         # [한계] "아무리 바빠도 0.5코어 이상은 쓰지마" (강제 제한)
      #       memory: "256Mi"

# ====================================================================================================







# ####################################################################################################
# III. 실행
# ====================================================================================================
# 1. EKS 클러스터 및 노드 그룹 생성
# ----------------------------------------------------------------------------------------------------
eksctl create cluster -f create_cluster.yaml

# (필요할 경우)노드 그룹의 노드 수 조정
eksctl scale nodegroup \
    --cluster=csjin-cluster \
    --name=csjin-cluster-ng \
    --nodes=2 \
    --nodes-min=1 \
    --nodes-max=3

# 현재 리전에 생성된 EKS 클러스터 목록과 상태 확인
eksctl get cluster [--name csjin-cluster]
# 인프라 관점에서의 노드 그룹 상태 및 개수 확인
kubectl get nodes [-o wide]
eksctl get nodegroup --cluster csjin-cluster
# AWS CloudFormation 스택 상태(인프라 구성 완료 여부) 확인
eksctl utils describe-stacks --cluster csjin-cluster > cloudformation-stacks.txt
# 사용중인 Local PC와 클러스터 간의 연결정보를 업데이트
aws eks --region ap-northeast-1 update-kubeconfig --name csjin-cluster
kubectl get nodes -o wide

# ====================================================================================================
# 2. 로드밸런서를 자동 생성하기 위한 작업
# ----------------------------------------------------------------------------------------------------
# 1) 역할담당자(신분증) 생성: IAM OIDC 프로바이더 연결 확인
# 실제 로드밸런서 생성을 위해 담당할 역할자의 역할 생성 :  여러번 실행 대비(--override-existing-serviceaccounts)
# IAM Service Account (IRSA: IAM Roles for Service Accounts)를 생성하여 ALB 컨트롤러에 권한 부여합니다.
# IRSA는 쿠버네티스 파드에게 AWS 리소스를 다룰 수 있는 신분증(권한)을 부여하는 것입니다.
# AWS Load Balancer Controller가 ALB를 생성하고 관리할 수 있도록 AmazonEC2FullAccess 정책을 연결한
# IAM 역할자를 kube-system 네임스페이스에 aws-load-balancer-controller 이름으로 생성합니다.
# 이후 로드밸런서를 생성할 때 이 aws-load-balancer-controller 서비스 계정을 사용하게 됩니다.
eksctl create iamserviceaccount \
  --cluster=csjin-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --attach-policy-arn=arn:aws:iam::aws:policy/AmazonEC2FullAccess \
  --override-existing-serviceaccounts \
  --approve

# 참고) iamserviceaccount 삭제 명령
eksctl delete iamserviceaccount \
  --cluster=csjin-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller

# 2) Helm 차트로 AWS Load Balancer Controller 설치
# Helm이 설치되어 있지 않다면 먼저 설치합니다.
# Helm 설치: https://helm.sh/docs/intro/install/
# 역할자를 생성하기 위한 AWS EKS 차트 리포지토리 추가
helm repo add eks https://aws.github.io/eks-charts

# 최신 정보로 업데이트
helm repo update

# AWS Load Balancer Controller 설치
helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=csjin-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=ap-northeast-1 \
  --set vpcId=vpc-089fe9578dcc1c00c

# 참고) 설치 제거 명령
# helm uninstall aws-load-balancer-controller -n kube-system

# 30초 정도 뒤에 설치 확인
kubectl get deployment -n kube-system aws-load-balancer-controller

# ====================================================================================================
# 3. 서비스 및 Ingress , Deployment 배포
# ----------------------------------------------------------------------------------------------------
# 배포 실행 명령: NodePort 버전
kubectl apply -f web-deployment-nodeport.yaml

# 배포 실행 명령: ClusterIP 버전
# kubectl apply -f web-deployment-clusterip.yaml

# 상태 확인
kubectl get svc web-project-svc -w
kubectl get ingress web-project-svc

# 로드밸런서 컨트롤러가 ALB를 잘 만들고 있는지 실시간 로그 확인
kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -f

# ####################################################################################################
# IV. 참고 사상
# 네트워크 드라이버를 NodePort로 사용할 경우 : 포트는 30000 ~ 32767 사이에서 자동 할당됩니다.
# --> 이경우 인스턴스에는 반드시 해당 포트를 위 범위로 열어주어야 합니다.
# 네트워크 드라이버를 ClusterIP로 사용할 경우 : 노드 포트가 아닌 클러스터 내부 IP로만 통신합니다.
# --> 이경우 인스턴스에는 서비스들의 정의된 포트들만 열려 있으면 됩니다.
# ####################################################################################################
