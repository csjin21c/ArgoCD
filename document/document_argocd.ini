# ####################################################################################################
# GigHub Actions & ArgoCD on EKS Cluster
# ####################################################################################################

# ####################################################################################################
# I. VPC Network Architecture
# ====================================================================================================
# 1. 리전 및 가용 영역 정보
# ----------------------------------------------------------------------------------------------------
# - Region: ap-northeast-1 (도쿄)
# - Availability Zones: ap-northeast-1a / ap-northeast-1c / ap-northeast-1d

# ====================================================================================================
# 2. VPC와 네트워크 기본정보
# ----------------------------------------------------------------------------------------------------
# Type            Availability Zone   Subnet ID                   CIDR 블록      비고
# ----------------------------------------------------------------------------------------------------
# vpc                                 vpc-089fe9578dcc1c00c       10.0.0.0/20   IGW
# ----------------------------------------------------------------------------------------------------
# Public Subnet   ap-northeast-1a     subnet-0b1805c1e9f180871    10.0.0.0/26   NAT,  외부 LB용
#                 ap-northeast-1c     subnet-0bb8d9b6a060a7b6b    10.0.1.0/26         외부 LB용
#                 ap-northeast-1d     subnet-0bc9e9327311ce319    10.0.2.0/26         외부 LB용
# ----------------------------------------------------------------------------------------------------
# Private Subnet  ap-northeast-1a     subnet-03d99036d72d4e535    10.0.0.64/26        내부 LB용
#                 ap-northeast-1c     subnet-0e961c601df663df9    10.0.1.64/26        내부 LB용
#                 ap-northeast-1d     subnet-0d9812b3f0d502b24    10.0.2.64/26        내부 LB용

# ====================================================================================================
# 3. VPC와 Subnet에 아래와 같이 태그를 추가합니다:
#    kubernetes.io/role/elb=1이 지정된 퍼블릭 서브넷은 외부 로드밸런서용 서브넷 대상이 되는 것으로,
# ----------------------------------------------------------------------------------------------------
# 대상 리소스          태그 키 (Key)                             태그 값 (Value)   비고
# ----------------------------------------------------------------------------------------------------
# VPC                kubernetes.io/cluster/[cluster-name]    shared          인프라 식별
# ----------------------------------------------------------------------------------------------------
# Public Subnet      kubernetes.io/cluster/[cluster-name]    shared          클러스터 소속
#                    kubernetes.io/role/elb                  1               외부 LB용
# ----------------------------------------------------------------------------------------------------
# Private Subnet     kubernetes.io/cluster/[cluster-name]    shared          클러스터 소속
#                    kubernetes.io/role/internal-elb         1               내부 LB용

# ====================================================================================================
# 4. 보안 그룹 설정
# ----------------------------------------------------------------------------------------------------
# Name              Secrets Group ID      인바운드 규칙
# ----------------------------------------------------------------------------------------------------
# csjin-default-sg  sg-025ab8cfad0cab78c  delete inbound
# csjin-ssh-sg      sg-04264d83ba971cc87  SSH(22)             User's IP
# csjin-alb-sg      sg-0dab199d320638d6a  HTTP(80)            0.0.0.0/0
#                                         HTTPS(443)          0.0.0.0/0
#                                         TCP(8000)           0.0.0.0/0
# csjin-cluster-sg  sg-0746271ec52f43ed1  TCP(30000 - 32767)  csjin-alb-sg
# csjin-mysql-sg    sg-082666b0cbf035f8e  MySQL(3306)         csjin-ssh-sg, csjin-cluster-sg

# ====================================================================================================
# 5. Application Load Balancer용 IAM Role 생성
# ----------------------------------------------------------------------------------------------------
# - 역할 이름: eksctl-csjin-cluster-Role1-1A2B2C3D4E5F
# - 정책 연결: AmazonEC2FullAccess

# ####################################################################################################
# II. EKS Cluster Creation and Management
# ====================================================================================================
# eksctl 클러스터 명령
# -------------------------------------------------------------------------------------------------------
#  단계       실행 도구                                    명령어 및 설명
#  최초 생성	 eksctl create cluster -f cluster.yaml	    YAML 설계도대로 클러스터와 노드를 처음 구축합니다.
#  설정 변경   eksctl upgrade nodegroup -f cluster.yaml   노드 그룹의 설정(버전, IAM 권한 등)을 수정하여 반영합니다.
#  개수 조정	 eksctl scale nodegroup -f cluster.yaml	    YAML에 정의된 min/max/desired 값으로 노드 대수를 맞춥니다.
#  삭제	      eksctl delete cluster -f cluster.yaml	     클러스터와 관련 리소스를 모두 삭제합니다.
# -------------------------------------------------------------------------------------------------------

# ====================================================================================================
# create_cluster.yaml 내용:
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

# 클러스터 기본 정보 설정
metadata:
  name: csjin-cluster        # 생성할 EKS 클러스터 이름 (태그와 일치해야 함)
  region: ap-northeast-1     # 클러스터가 생성될 리전 (도쿄)
  version: "1.31"            # 쿠버네티스 제어 평면 버전(1.29 ~ 1.35 지원, 1.31 권장)

# 기존 VPC 및 서브넷 활용 설정
vpc:
  id: "vpc-089fe9578dcc1c00c"  # 사용자가 UI로 생성한 기존 VPC ID
  subnets:
    # 외부 로드밸런서가 위치할 퍼블릭 서브넷 (elb 태그 필요)
    public:
      ap-northeast-1a: { id: "subnet-0b1805c1e9f180871" }
      ap-northeast-1c: { id: "subnet-0bb8d9b6a060a7b6b" }
      ap-northeast-1d: { id: "subnet-0bc9e9327311ce319" }
    # 실제 워커 노드가 위치할 프라이빗 서브넷 (internal-elb 태그 필요)
    private:
      ap-northeast-1a: { id: "subnet-03d99036d72d4e535" }
      ap-northeast-1c: { id: "subnet-0e961c601df663df9" }
      ap-northeast-1d: { id: "subnet-0d9812b3f0d502b24" }
  # VPC 서브넷 태그 자동 생성을 위한 설정 (Public 서브넷 사용)
  clusterEndpoints:
    publicAccess: true # 보안 관점에선 false 권장, 하지만 CI/CD 환경에선 true 필요(외부 접속 허용)
    privateAccess: true
  # 특정 IP만 조종실(k8s master)에 접속 가능하도록 제한, publicAccess: true 시에만 유효하나 취약점 보완할 수 있어 사용합니다.
  # publicAccessCidrs:
  #   - "210.xxx.xxx.xxx/32"
  #   - "1.2.3.4/32"

# 관리형 노드 그룹(Managed Node Group) 설정
managedNodeGroups:
  - name: csjin-cluster-ng     # 노드 그룹의 이름
    instanceType: t3.medium    # 사용할 EC2 인스턴스 타입 (사양)
    minSize: 1                 # 최소 1대 유지
    maxSize: 3                 # 최대 3대까지 확장 가능
    desiredCapacity: 2         # 초기 생성할 노드 개수
    volumeSize: 20             # 각 노드의 EBS(디스크) 크기 (GB)
    privateNetworking: true    # 노드들을 외부 인터넷에 노출되지 않는 프라이빗 서브넷에 배치
    securityGroups:
      attachIDs:
        - sg-0746271ec52f43ed1 # 첫 번째 보안 그룹: csjin-cluster-sg
        - sg-04264d83ba971cc87 # 두 번째 보안 그룹: csjin-ssh-sg, 확인 후 삭제
    iam:
      # 노드에 기본적으로 필요한 AWS 정책 권한 추가
      withAddonPolicies:
        imageBuilder: true     # ECR 이미지 접근 권한
        autoScaler: true       # Cluster Autoscaler 사용을 위한 권한
        albIngress: true       # 로드밸런서 관련 기본 권한을 노드에 자동으로 부여
        cloudWatch: true       # 로그 전송을 위한 CloudWatch 권한
        ebs: true              # EBS 볼륨 사용 권한
        appMesh: true          # 서비스 메시 사용 시 필요
        xRay: true             # 분산 트레이싱 사용 시 필요
        certManager: true      # SSL/TLS 인증서 자동 발급 권한
        externalDNS: true      # 도메인(Route53) 자동 연결 권한

# IAM OIDC 프로바이더 설정
iam:
  withOIDC: true               # 쿠버네티스 서비스 계정에 IAM 역할을 부여하기 위한 설정 (필수)serviceAccounts:
    - metadata:
        name: argocd-manager
        namespace: argocd
      attachPolicyARNs:
        - "arn:aws:iam::aws:policy/ReadOnlyAccess" # 예시: 필요한 권한에 따라 조절
# ====================================================================================================



# ####################################################################################################
# III. 애플리케이션 배포: Nginx 웹서버 + FastAPI (로드밸런서 포함)
# ====================================================================================================
# 1. Service: 외부에서 접속 가능한 로드밸런서(대문) 정의 : web-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: web-project-svc      # 서비스 이름
spec:
  type: NodePort             # ALB는 NodePort 서비스를 바라보고 트래픽을 넘깁니다.
  selector:
    app: web-project                # 위에서 만든 nginx 라벨을 가진 파드들로 트래픽을 전달
  ports:
    - name: http               # [리스너 1] 이름 지정 (포트가 여러 개일 땐 필수)
      protocol: TCP
      port: 80                 # 로드밸런서가 외부에서 받는 포트
      targetPort: 80           # 실제 파드로 전달되는 포트
    # - name: custom-port        # [리스너 2] 추가된 리스너
    #   protocol: TCP
    #   port: 8000               # 외부에서 8080으로 접속하면
    #   targetPort: 8000           # 파드의 80번으로 전달 (포트 포워딩 역할)
---
# 2. Ingress를 통해 ALB를 생성합니다
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-project-svc         
  annotations: 
    # ALB 클래스 지정 (최신 방식은 spec.ingressClassName을 쓰지만 annotation도 작동함)
    kubernetes.io/ingress.class: alb
    
    # 외부 노출 및 타겟 설정
    alb.ingress.kubernetes.io/scheme: internet-facing   # internet-facing(외부공개)/internal(내부)
    alb.ingress.kubernetes.io/target-type: instance      # instance 또는 ip(파드 직접 지정, VPC CNI환경이 완벽해야 함)
    
    # 보안 그룹
    alb.ingress.kubernetes.io/security-groups: sg-0dab199d320638d6a
    
    # 4. 헬스체크 설정
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    
    # 기타 속성 (ALB 전용 표기법)
    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=60
spec:
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-project-svc   # 서비스 이름과 일치해야 함
                port:
                  number: 80
---
# ---------------------------------------------------------
# 2. Deployment: 애플리케이션(Nginx) 컨테이너 실행 정의
# ---------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-deployment      # 이 배포의 이름
spec:
  replicas: 2                 # [요청 개수] 처음에 띄울 파드의 개수 (요청하신 2개)
  selector:
    matchLabels:
      app: web-project              # 이 라벨이 붙은 파드들을 관리하겠다는 선언
  template:
    metadata:
      labels:
        app: web-project            # 생성될 파드에 부여할 라벨 (위의 selector와 일치해야 함)
    spec:
      containers:
      # 1번 컨테이너: Nginx (웹서버 역할)
      - name: nginx
        image: 626635419731.dkr.ecr.ap-northeast-1.amazonaws.com/csjin/nginx:latest-aws
        ports:
        - containerPort: 80    # 컨테이너 안에서 Nginx가 사용하는 포트
        resources:             # [중요] HPA가 CPU 사용량을 계산하기 위한 기준 설정
          requests:
            cpu: "100m"         # [예약] "최소한 이만큼은 보장해줘" (스케줄링 기준)
            memory: "128Mi"
          limits:
            cpu: "250m"         # [한계] "아무리 바빠도 0.5코어 이상은 쓰지마" (강제 제한)
            memory: "256Mi"
      
      # 2번 컨테이너: FastAPI (Application 역할)
      # - name: fastapi
      #   image: 626635419731.dkr.ecr.ap-northeast-1.amazonaws.com/csjin/fastapi:3.14-slim
      #   ports:
      #   - containerPort: 8000  # FastAPI가 사용하는 포트
      #   resources:
      #     requests:
      #       cpu: "100m"         # [예약] "최소한 이만큼은 보장해줘" (스케줄링 기준)
      #       memory: "128Mi"
      #     limits:
      #       cpu: "500m"         # [한계] "아무리 바빠도 0.5코어 이상은 쓰지마" (강제 제한)
      #       memory: "256Mi"

# ====================================================================================================







# ####################################################################################################
# III. 실행 - EKS 클러스터 생성 및 애플리케이션 배포
# ====================================================================================================
# 1. EKS 클러스터 및 노드 그룹 생성
# ----------------------------------------------------------------------------------------------------
eksctl create cluster -f create_cluster.yaml

# (필요할 경우)노드 그룹의 노드 수 조정
eksctl scale nodegroup \
    --cluster=csjin-cluster \
    --name=csjin-cluster-ng \
    --nodes=2 \
    --nodes-min=1 \
    --nodes-max=3

# 현재 리전에 생성된 EKS 클러스터 목록과 상태 확인
eksctl get cluster [--name csjin-cluster]
# 인프라 관점에서의 노드 그룹 상태 및 개수 확인
kubectl get nodes [-o wide]
eksctl get nodegroup --cluster csjin-cluster
# AWS CloudFormation 스택 상태(인프라 구성 완료 여부) 확인
eksctl utils describe-stacks --cluster csjin-cluster > cloudformation-stacks.txt
# 사용중인 Local PC와 클러스터 간의 연결정보를 업데이트
aws eks --region ap-northeast-1 update-kubeconfig --name csjin-cluster
kubectl get nodes -o wide

# ====================================================================================================
# 2. 로드밸런서를 자동 생성하기 위한 작업
# ----------------------------------------------------------------------------------------------------
# 1) 역할담당자(신분증) 생성: IAM OIDC 프로바이더 연결 확인
# 실제 로드밸런서 생성을 위해 담당할 역할자의 역할 생성 :  여러번 실행 대비(--override-existing-serviceaccounts)
# IAM Service Account (IRSA: IAM Roles for Service Accounts)를 생성하여 ALB 컨트롤러에 권한 부여합니다.
# IRSA는 쿠버네티스 파드에게 AWS 리소스를 다룰 수 있는 신분증(권한)을 부여하는 것입니다.
# AWS Load Balancer Controller가 ALB를 생성하고 관리할 수 있도록 AmazonEC2FullAccess 정책을 연결한
# IAM 역할자를 kube-system 네임스페이스에 aws-load-balancer-controller 이름으로 생성합니다.
# 이후 로드밸런서를 생성할 때 이 aws-load-balancer-controller 서비스 계정을 사용하게 됩니다.
eksctl create iamserviceaccount \
  --cluster=csjin-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --attach-policy-arn=arn:aws:iam::aws:policy/AmazonEC2FullAccess \
  --override-existing-serviceaccounts \
  --approve

# 참고) iamserviceaccount 삭제 명령
eksctl delete iamserviceaccount \
  --cluster=csjin-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller

# 2) Helm 차트로 AWS Load Balancer Controller 설치
# Helm이 설치되어 있지 않다면 먼저 설치합니다.
# Helm 설치: https://helm.sh/docs/intro/install/
# 역할자를 생성하기 위한 AWS EKS 차트 리포지토리 추가
helm repo add eks https://aws.github.io/eks-charts

# 최신 정보로 업데이트
helm repo update

# AWS Load Balancer Controller 설치
helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=csjin-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=ap-northeast-1 \
  --set vpcId=vpc-089fe9578dcc1c00c

# 참고) 설치 제거 명령
# helm uninstall aws-load-balancer-controller -n kube-system

# 30초 정도 뒤에 설치 확인
kubectl get deployment -n kube-system aws-load-balancer-controller

# ====================================================================================================
# 3. 서비스 및 Ingress , Deployment 배포
# ----------------------------------------------------------------------------------------------------
# 배포 실행 명령: NodePort 버전
kubectl apply -f web-deployment-nodeport.yaml

# 배포 실행 명령: ClusterIP 버전
# kubectl apply -f web-deployment-clusterip.yaml

# 상태 확인
kubectl get svc web-project-svc -w
kubectl get ingress web-project-svc

# 로드밸런서 컨트롤러가 ALB를 잘 만들고 있는지 실시간 로그 확인
kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -f

# ####################################################################################################
# IV. 참고 사상
# 네트워크 드라이버를 NodePort로 사용할 경우 : 포트는 30000 ~ 32767 사이에서 자동 할당됩니다.
# --> 이경우 인스턴스에는 반드시 해당 포트를 위 범위로 열어주어야 합니다.
# 네트워크 드라이버를 ClusterIP로 사용할 경우 : 노드 포트가 아닌 클러스터 내부 IP로만 통신합니다.
# --> 이경우 인스턴스에는 서비스들의 정의된 포트들만 열려 있으면 됩니다.

# 현재 클러스터 상태 확인
aws eks describe-cluster --name <클러스터명> --query "cluster.status"

# 현재 작업 클러스터 확인
kubectl config current-context

# 클러스터 변경: kubectl 명령 대상 클러스터 변경
aws eks update-kubeconfig --region <리전명> --name <클러스터명>
# ####################################################################################################




# ####################################################################################################
# V. GitHb Action 워크플로우 구성( CI )
# ====================================================================================================
 
# 1. GhiHub Repository 생성: https://github.com/csjin21c/ArgoCD.git
# 2. GitHub Secrets 설정
#    - AWS_ACCESS_KEY_ID: <액세스 키 ID>
#    - AWS_SECRET_ACCESS_KEY: <비밀 액세스 키>
# 3. 프로젝트 디렉토리 구조
.
├── fastapi
│   ├── app.py
│   ├── requirements.txt
│   └── test_app.py
├── k8s
│   └── web-deployment-nodeport.yaml
└── nginx
    ├── default.conf
    ├── Dockerfile
    └── html
        ├── images
        │   └── nginx.png
        └── index.html

├── fastapi
│   ├── Dockerfile
│   ├── main.py
│   └── requirements.txt
├── k8s
│   └── web-deployment-nodeport.yaml
├── nginx
│   ├── default.conf
│   ├── Dockerfile
│   └── html
│       ├── images
│       │   └── nginx.png
│       └── index.html
├── old_document.ini
└── README.md
# ====================================================================================================
# 4. .github/workflows/argocd-ci.yaml
# ----------------------------------------------------------------------------------------------------
name: Build and Push to ECR

on:
  push:
    branches: ["main"]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & Push FastAPI
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/csjin/fastapi:$IMAGE_TAG ./fastapi
          docker push $ECR_REGISTRY/csjin/fastapi:$IMAGE_TAG

      - name: Build & Push Nginx
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/csjin/nginx:$IMAGE_TAG ./nginx
          docker push $ECR_REGISTRY/csjin/nginx:$IMAGE_TAG
#


# ####################################################################################################
# VI. ArgoCD 설치
# ====================================================================================================

# 1. ArgoCD를 위한 별도의 방(Namespace) 만들기
kubectl create namespace argocd

# 2. 공식 설치 스크립트 실행
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 3. ArgoCD 서버 외부 노출 (접속하기)
# 설치 직후에는 ArgoCD 대시보드가 클러스터 내부에 숨어있습니다. 관리자가 웹 브라우저로 접속할 수 있도록 서비스를 로드밸런서 타입으로 변경해줍니다.
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# 4. 서비스 타입을 LoadBalancer로 변경
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# 5. 서버를 --insecure 모드로 변경: 기본적으로 HTTPS로 구동되게 되어있으나 아직 인증서가 없는 관계로 HTTP 프로토콜로 구동 될 수 있도록 수정
kubectl patch deployment argocd-server -n argocd --type='json' \
    -p='[{"op": "add", "path": "/spec/template/spec/containers/0/command", "value": ["argocd-server", "--insecure"]}]'

# 6. 아래 명령어로 접속 주소(External IP)가 생성될 때까지 기다렸다가 확인합니다.
kubectl get svc argocd-server -n argocd

# 7. ArgoCD 로그인 아이디는 admin입니다. 비밀번호는 아래 명령어로 추출할 수 있는 초기 생성값이니 메모해 두세요.
kubectl -n argocd get secret argocd-initial-admin-secret \
    -o jsonpath="{.data.password}" | base64 -d # admin / jRcwhtfKDEqTz713 ==> 맥의 경우 뒤에 %는 삭제

# 8. ArgoCD UI에서 비밀번호 수정: User Info > Update Password

# 참고 ----------
# ArgoCD 삭제: 문제가 발생할 경우 아르고씨디를 삭제후 재시도
# 설치할 때 썼던 매니페스트를 이용해 역으로 삭제
kubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 네임스페이스까지 완전히 삭제 (시간이 좀 걸릴 수 있습니다)
kubectl delete namespace argocd

# 비밀번호를 'admin123'으로 변경하는 명령어
kubectl -n argocd patch secret argocd-initial-admin-secret -p "{\"data\": {\"password\": \"$(echo -n 'admin1234!' | base64)\"}}"

# ArgoCD 재식작
kubectl rollout restart deployment argocd-server -n argocd
# ----------




# ####################################################################################################
# VII. ArgoCD 배포 설정
# ====================================================================================================

# 1. GitHub Repository 정보 등록
# ArgoCD Setting > Repositories > CONNECT REPO 
# 주요 입력 사항
# ----------------------------------------------------------------------------------------------------
# 항목                          내용
# ----------------------------------------------------------------------------------------------------
# Type	                        git (기본값)
# Name	                        ArgoCD 내부에서 리포지토리를 식별하는 별명입니다.(예. csjin-eks-repo)
# Project	                      default (드롭다운에서 선택하거나 직접 입력)
# Repository URL	              준비하신 GitHub 주소 (예: https://github.com/csjin21c/ArgoCD.git)
# Username	                    csjin21c (GitHub 계정명)
# Password	                    [중요] GitHub  **Personal Access Token(PAT)**
# Bearer token	                비트버킷(Bitbucket) 등 특정 서비스에서만 쓰는 방식입니다.
# TLS client certificate	      자체 서명된 인증서를 쓰는 기업 내부망 환경이 아니면 필요 없습니다.
# TLS client certificate key	  위 항목과 세트이며, 일반적인 GitHub 연동에는 쓰지 않습니다.
# ----------------------------------------------------------------------------------------------------
# 1-1. GitHub Personal Access Token(PAT) 발급
# 발급순서: 
#   GitHub > GitHub 사용자 Icon > Setting > (사이드 메뉴 최 하단)Developer settings > Personal access tokens
#   > Tokens (classic) > Generate new token > Generate new tonen (classic) > Verify via email 
#   > 메일에서 인증번호 확인 > 입력 및 Verify
#   New personal access token (classic)
#    - Note: ArgoCD-Token (나중에 알아보기 쉽게 작성)
#    - Expiration(만료일): No expiration (학습용으로 사용을 위해 만료 기한 없음을 선택)
#    - Select scopes: repo 체크, ArgoCD가 GitHub에 있는 소스 코드를 읽어오려면 이 권한이 반드시 필요합니다.
#   화면 하단 "Generate token" 버튼 클릭 => token 복사

# 1-2. GitHub Actions에게 Repository로 Push할 수 있는 권한 부여
# GitHub Repository > Setting > (Side menu)Actions > General
# - 페이지 하단 Workflow permissions를 "Read and write permissions"로 수정
# - Allow GitHub Actions to create and approve pull requests에 체크
#    --> 이체크를 통해 단순히 파일 수정을 넘어 "Pull Request(PR)"를 직접 생성하거나 승인할 수 있는 권한까지 부여하는 것입니다.


# 2. Application 만들기
# 생성 순서:
#   (Side Menu)Applications > "NEW APP" > 아래 내용 입력 > "CREATE"
# 1) GENERAL (일반 설정)
# - Application Name: 애플리케이션 이름(예. csjin-eks-app)
# - Project: `default`
# - Sync Policy: `Automatic`을 선택하면 Git에 push만 해도 자동으로 배포가 됩니다.
# 2) SOURCE (소스 설정)
# - Repository URL: 등록한 GitHub Repository를 선택합니다.
# - Revision: `HEAD` (기본값)
# - Path: [중요] 생성할 Application(POD)관련 메니페스트(YAML) 파일이 있는 위치를 지정합니다.(예. k8s)
# 3) DESTINATION (목적지 설정)
# - Cluster URL: https://kubernetes.default.svc 선택
# - Namespace: default 선택 (혹은, 배포를 원하는 특정 네임스페이스)
#     - 여기서는 default로 놓는 것이 좋습니다.
#     - 메니페스트 파일에 네임스페이스를 정의 했을 경우 우선하게 됩니다.
#     - 메니페스트 파일의 네임스페이스와 ArgoCD의 이곳 네임스페이스가 다를 경우 메니페이스를 우선하게 됩니다. 다만, 이때 `OutOfSync(비동기)`경고를 띄울 수 있습니다.
# 4) Directory (생략) : Directory 섹션은 일반적으로 정의하지 않습니다. Path 항목에서 정의된 내용으로 대체 됩니다.
#   [ Directory 역할 ]
#   - Recures (재귀적 탐색): 만약 k8s 폴더 안에 또 다른 하위 폴더들이 있다면, 이 옵션을 체크해야 하위 폴더의 YAML까지 모두 배포하게 됩니다. 다만, Path에서 지정한 폴더 바로 아래 파일만 대상이 됩니다.
#   - 특정 파일 제외 (Exclude): 폴더 안에 배포하고 싶지 않은 설정 파일(예: `README.md`나 임시 파일)이 섞여 있을 때, 특정 패턴을 가진 파일을 배포에서 제외할 수 있습니다.
#   - JSON/YAML 외 포맷 지원: 기본적으로 `.yaml`, `.yml`, `.json` 파일만 읽지만, 설정에 따라 다른 형식의 매니페스트를 처리하도록 커스텀할 때 사용합니다.






# 깃 명령
# 1. 머지(Merge) 방식을 기본으로 사용하도록 설정(처음에만)
git config pull.rebase false

# 2. 다시 한번 pull 시도
# 이때 커밋 메시지를 적으라는 편집기(Vim 등)가 뜰 수 있습니다. 
# 메시지가 뜨면 ':wq'를 입력해 저장하고 나오시면 됩니다.
git pull origin main

# 3. 내 변경 사항(Nginx 설정) 스테이징 및 커밋
git add .
git commit -m "Fix: Update Nginx configuration for /api proxy"

# 4. 최종 Push
git push origin main


